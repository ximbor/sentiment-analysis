name: model-deploy

on:
  schedule:
  - cron: '0 3 * * 1'
  
  workflow_dispatch:
    inputs:
      dataset_train_size:
        description: 'Number of training samples (0 for all)'
        required: false
        default: '0'
      dataset_test_size:
        description: 'Number of testing samples (0 for all)'
        required: false
        default: '0'
      learning_rate:
        description: 'Learning rate (e.g., 2e-5)'
        required: false
        default: '2e-5'
      train_epochs:
        description: 'Number of training epochs'
        required: false
        default: '3'
      train_batch_size:
        description: 'Batch size'
        required: false
        default: '8'

jobs:
  train_and_validate:
    runs-on: ubuntu-latest
    outputs:
      should_deploy: ${{ steps.metrics_check.outputs.improved_metrics }}
    steps:
    - name: Checkout Code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install transformers[torch] datasets scikit-learn pandas

    - name: Run Retraining and Validation
      env:
        DATASET_TRAIN_SIZE: ${{ github.event.inputs.dataset_train_size || '0' }}
        DATASET_TEST_SIZE: ${{ github.event.inputs.dataset_test_size || '0' }}
        LEARNING_RATE: ${{ github.event.inputs.learning_rate || '2e-5' }}
        TRAIN_EPOCHS: ${{ github.event.inputs.train_epochs || '3' }}
        TRAIN_BATCH_SIZE: ${{ github.event.inputs.train_batch_size || '8' }}
      run: python scripts/retrain.py

    - name: Generate Performance Summary
      id: metrics_check
      run: |
        source metrics.env
        echo "### ðŸ“Š Metrics report" >> $GITHUB_STEP_SUMMARY
        echo "| Metric | Deployed model | Model tested " >> $GITHUB_STEP_SUMMARY
        echo "| :--- | :---: | :---: " >> $GITHUB_STEP_SUMMARY
        echo "| **F1-Macro** | $BASELINE_F1 | $NEW_F1 " >> $GITHUB_STEP_SUMMARY
        echo "| **Accuracy** | $BASELINE_ACC | $NEW_ACC " >> $GITHUB_STEP_SUMMARY
        echo "improved_metrics=IMPROVED_METRICS" >> $GITHUB_OUTPUT

    - name: Install test dependencies
      if: steps.metrics_check.outputs.improved_metrics == 'true'
      run: |
        pip install pytest pytest-asyncio httpx pydantic uvicorn fastapi

    - name: Run tests
      if: steps.metrics_check.outputs.improved_metrics == 'true'
      env:
        PYTHONPATH: .
        MODEL_NAME: "./tmp_model"
      run: |
        pytest -v -s tests/test_main.py

    - name: Upload Model Artifact
      if: steps.metrics_check.outputs.improved_metrics == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: trained-model-files
        path: tmp_model/
        retention-days: 1

  deploy:
    needs: train_and_validate
    runs-on: ubuntu-latest
    if: |
          github.ref == 'refs/heads/main' && 
          needs.train_and_validate.outputs.improved_metrics == 'true'
    steps:
    - name: Checkout Code
      uses: actions/checkout@v3

    - name: Download Model Artifact
      uses: actions/download-artifact@v4
      with:
        name: trained-model-files
        path: tmp_model/

    - name: Install HF Hub
      run: pip install huggingface_hub

    - name: Push to HuggingFace Spaces/Hub
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        python -c "
        from huggingface_hub import HfApi
        api = HfApi()
        api.upload_folder(
            folder_path='./tmp_model',
            repo_id='ximbor/sentiment-monitor',
            repo_type='model',
            token='${{ secrets.HF_TOKEN }}'
        )"
